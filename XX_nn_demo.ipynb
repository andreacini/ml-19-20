{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XX_nn_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreacini/ml-19-20/blob/master/XX_nn_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aNZcctTjWk4h"
      },
      "source": [
        "# Machine Learning \n",
        "\n",
        "Prof. Cesare Alippi   \n",
        "Andrea Cini ([`andrea.cini@usi.ch`](mailto:andrea.cini@usi.ch))   \n",
        "Daniele Zambon ([`daniele.zambon@usi.ch`](mailto:daniele.zambon@usi.ch))   \n",
        "\n",
        "---\n",
        "\n",
        "# Training a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yCTjYu3dVrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from IPython import display\n",
        "from tqdm import tqdm\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "import time\n",
        "\n",
        "# color_maps\n",
        "cm = plt.cm.RdBu\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDez5u7Vmsnh",
        "colab_type": "text"
      },
      "source": [
        "## 1. Regression problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lI1zihSzWk4p",
        "colab": {}
      },
      "source": [
        "def get_data(n, rang, sigma):\n",
        "    \"\"\" Generate data for regression. \"\"\"\n",
        "    x_true = np.linspace(*rang, 1000)\n",
        "    y_true = g(x_true)\n",
        "\n",
        "    # Random data\n",
        "    x_data = np.random.rand(n)*(rang[1] - rang[0]) + rang[0]\n",
        "    noise = np.random.randn(n)*sigma\n",
        "    y_data = g(x_data) + noise\n",
        "\n",
        "    # normalization\n",
        "    xmu = x_data.mean()\n",
        "    xstd = x_data.std() \n",
        "    normalize = lambda x: (x-xmu)/xstd\n",
        "    return x_true, y_true, x_data, y_data, normalize\n",
        "\n",
        "def get_regressor(neurons, patience):\n",
        "    \"\"\" Generate the regressive model. \"\"\"\n",
        "    inp = keras.Input((1,))\n",
        "    hid1 = keras.layers.Dense(neurons, activation=\"tanh\")(inp)\n",
        "    # hid2 = keras.layers.Dense(10, activation=\"tanh\")(hid1)\n",
        "    out = keras.layers.Dense(1, activation=\"linear\")(hid1)\n",
        "    model = keras.Model(inp, out)\n",
        "    model.compile(loss=keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam())\n",
        "    es = keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=1, patience=patience)\n",
        "    return model, es\n",
        "\n",
        "def run_regression_demo(n, rang, sigma, neurons, patience, epochs_per_iter, max_epochs):\n",
        "    \"\"\" Run the demo. \"\"\"\n",
        "    #get data\n",
        "    x_true, y_true, x_data, y_data, normalize = get_data(n=n, rang=rang, sigma=sigma)\n",
        "    x_train, y_train = normalize(x_data), y_data\n",
        "    #get model\n",
        "    model, es = get_regressor(neurons=neurons, patience=patience)\n",
        "\n",
        "    #init figure\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca()\n",
        "    #fixed plots\n",
        "    ax.plot(x_true, g(x_true), c=\"r\", label=\"g(x)\")\n",
        "    ax.scatter(x_data, y_data, c=\"k\", label=\"data\")\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "    ax.set_xlim([rang[0]*.9, rang[1]*1.1])\n",
        "    ax.set_ylim([min([0, y_data.min()])-.1, min([.6, y_data.max()+.1])])\n",
        "    y_pred = model.predict(normalize(x_true))\n",
        "    line=ax.plot(x_true, y_pred, c=\"g\", label=\"NN(x)\")[0]\n",
        "    ax.legend(loc='upper right')\n",
        "    display.display(plt.gcf(), display_id=40)\n",
        "\n",
        "    #cycle training\n",
        "    niter = int(max_epochs/epochs_per_iter)\n",
        "    for i in tqdm(range(niter),disable=True):\n",
        "        # train\n",
        "        history = model.fit(x_train, y_train, epochs=epochs_per_iter, callbacks=[es], verbose=0)\n",
        "        y_pred = model.predict(normalize(x_true))\n",
        "\n",
        "        # update plot\n",
        "        line.set_ydata(y_pred)   \n",
        "        ax.set_title(\"seed: {} | epochs: {}/{}\".format(seed,\n",
        "                                            epochs_per_iter*i+len(history.history[\"loss\"]), \n",
        "                                            epochs_per_iter*niter))   \n",
        "        display.update_display(plt.gcf(), display_id=40)\n",
        "\n",
        "        # early stopping\n",
        "        if len(history.history[\"loss\"]) < epochs_per_iter:\n",
        "            break      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2go0dkeRdkAx",
        "colab_type": "text"
      },
      "source": [
        "Here you can play with the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_H6tl8ATWk47",
        "colab": {}
      },
      "source": [
        "# Regression problem\n",
        "N = 20                 # num datapoints\n",
        "SIGMA = 0.03           # noise std\n",
        "RANGE = [2., 20.]      # range of function g(x)\n",
        "def g(x):              # true function\n",
        "    return (x**2) / (x**3 + 1) + np.sin(x) / (x+10)\n",
        "\n",
        "# Regressor\n",
        "NEURONS = 5            # num neurons\n",
        "PATIENCE = 40          # patience early stopping\n",
        "MAX_EPOCHS = 1e5       # max num training epochs\n",
        "\n",
        "# Plot\n",
        "EPOCHS_PER_ITER = 400  # update plot every...\n",
        "\n",
        "#seed = 42\n",
        "seed = np.random.randint(1000000)\n",
        "np.random.seed(seed)\n",
        "run_regression_demo(n=N, rang=RANGE, sigma=SIGMA, neurons=NEURONS, \n",
        "                    patience=PATIENCE, epochs_per_iter=EPOCHS_PER_ITER, max_epochs=MAX_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFIWwZl3mxl2",
        "colab_type": "text"
      },
      "source": [
        "## 2. Classification problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTtJ0darc4Th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_spirals(n_points, noise=.01):\n",
        "    \"\"\"\n",
        "     Returns the two spirals dataset.\n",
        "     from:https://glowingpython.blogspot.com/2017/04/solving-two-spirals-problem-with-keras.html\n",
        "    \"\"\"\n",
        "    n = np.sqrt(np.random.rand(n_points,1)) * 500 * (2*np.pi)/360\n",
        "    d1x = - np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
        "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
        "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n",
        "            np.hstack((np.zeros(n_points),np.ones(n_points))))\n",
        "    \n",
        "def get_classifier(neurons, patience):\n",
        "    inp = keras.Input((2,))\n",
        "    hid1 = keras.layers.Dense(neurons, activation=\"tanh\")(inp)\n",
        "    out = keras.layers.Dense(1, activation=\"sigmoid\")(hid1)\n",
        "    model = keras.Model(inp, out)\n",
        "    model.compile(loss=keras.losses.BinaryCrossentropy(), \n",
        "                  optimizer=keras.optimizers.Adam(), \n",
        "                  metrics=['accuracy'])\n",
        "    es = keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=1, patience=patience)\n",
        "    return model, es\n",
        "            \n",
        "def run_classification_demo(n, noise, typ, neurons, patience, epochs_per_iter, max_epochs):\n",
        "    if typ == 'circles':\n",
        "        data_gen = lambda: make_circles(n, noise=noise, factor=0.5)\n",
        "    elif typ == 'moons':\n",
        "        data_gen = lambda : make_moons(n, noise=noise)\n",
        "    elif typ == 'simple':\n",
        "        def data_gen():\n",
        "            X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1)\n",
        "            X += noise * 10 * np.random.uniform(size=X.shape)\n",
        "            return (X, y)\n",
        "    elif typ == 'spirals':\n",
        "        data_gen = lambda : make_spirals(n, noise)\n",
        "    \n",
        "    X, y = data_gen()\n",
        "    normalize = lambda x: (x- x.mean(axis=0, keepdims=True))/x.std(axis=0, keepdims=True)\n",
        "    \n",
        "    X = normalize(X)\n",
        "    \n",
        "    #get model\n",
        "    model, es = get_classifier(neurons=neurons, patience=patience)\n",
        "    \n",
        "    #init figure\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca()\n",
        "\n",
        "    # Create mesh\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # plot train data\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,\n",
        "               edgecolors='k')\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    \n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "    \n",
        "    y_pred = model.predict(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
        "    \n",
        "    y_pred = y_pred.reshape(xx.shape)\n",
        "    surface = ax.contourf(xx, yy, y_pred, levels=np.linspace(0,1,8), cmap=cm, alpha=.8)\n",
        "    cbar = fig.colorbar(surface, ax=ax)\n",
        "    \n",
        "    display.display(plt.gcf(), display_id=42)\n",
        "\n",
        "    #cycle training\n",
        "    niter = int(max_epochs/epochs_per_iter)\n",
        "    for i in tqdm(range(niter),disable=True):\n",
        "        # train\n",
        "        history = model.fit(X, y, epochs=epochs_per_iter, callbacks=[es], verbose=0)\n",
        "        y_pred = model.predict(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
        "        y_pred = y_pred.reshape(xx.shape)\n",
        "        \n",
        "        for coll in surface.collections: \n",
        "            plt.gca().collections.remove(coll) \n",
        "        surface = ax.contourf(xx, yy, y_pred, cmap=cm, alpha=.8)\n",
        "\n",
        "        ax.set_title(\"seed: {} | epochs: {}/{}\".format(seed,\n",
        "                                           epochs_per_iter*i+len(history.history[\"loss\"]), \n",
        "                                           epochs_per_iter*niter))   \n",
        "        display.update_display(plt.gcf(), display_id=42)\n",
        "\n",
        "        # early stopping\n",
        "        if len(history.history[\"loss\"]) < epochs_per_iter:\n",
        "            break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORvvRbELdy2j",
        "colab_type": "text"
      },
      "source": [
        "Here you can play with the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OJAXDuTdX8on",
        "colab": {}
      },
      "source": [
        "# Classification problem\n",
        "N = 100                 # num datapoints\n",
        "NOISE = 0.1             # noise\n",
        "TYPE = 'circles'        # choose between 'circles', 'moons', 'spirals', 'simple'\n",
        "\n",
        "# Classifier\n",
        "NEURONS = 20           # num neurons\n",
        "PATIENCE = 40          # patience early stopping\n",
        "EPOCHS_MAX = 4000      # max num training epochs\n",
        "# Plot\n",
        "EPOCHS_PER_ITER = 50  # update plot every...\n",
        "\n",
        "seed = 100\n",
        "np.random.seed(seed)\n",
        "run_classification_demo(n=N, noise=NOISE, typ=TYPE, neurons=NEURONS, patience=PATIENCE, \n",
        "                        epochs_per_iter=EPOCHS_PER_ITER, max_epochs=EPOCHS_MAX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BHyJ9HLTmVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}